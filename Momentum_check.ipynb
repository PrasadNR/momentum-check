{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgEc9RYwdNyS"
   },
   "source": [
    "## Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UcVJ89lWfBqk"
   },
   "source": [
    "Source: https://github.com/akshat57/Blind-Descent/blob/main/Blind_Descent-1-CNN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "executionInfo": {
     "elapsed": 394,
     "status": "ok",
     "timestamp": 1611975282848,
     "user": {
      "displayName": "Prasad Narahari Raghavendra",
      "photoUrl": "",
      "userId": "13004936298388256166"
     },
     "user_tz": 300
    },
    "id": "T5oQ47tOdNyU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "cuda = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2ZlVqRvdNyV"
   },
   "source": [
    "## Download the MNIST and CIFAR10 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2734,
     "status": "ok",
     "timestamp": 1611975285196,
     "user": {
      "displayName": "Prasad Narahari Raghavendra",
      "photoUrl": "",
      "userId": "13004936298388256166"
     },
     "user_tz": 300
    },
    "id": "snx5udk9dNyW",
    "outputId": "80f7f641-a9f0-4c8b-dc81-2d0d1c5296a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "MNIST is already an array\n",
      "torch.Size([60000, 28, 28]) torch.Size([60000]) torch.Size([10000, 28, 28]) torch.Size([10000])\n",
      "\n",
      "CIFAR10 is a list of arrays\n",
      "50000 50000 10000 10000\n",
      "(32, 32, 3) (32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "train = MNIST('./MNIST_data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test = MNIST('./MNIST_data', train=False, download=True, transform=transforms.ToTensor())\n",
    "train_MNIST_data = train.data; train_MNIST_labels = train.targets\n",
    "test_MNIST_data = test.data; test_MNIST_labels = test.targets\n",
    "\n",
    "train = CIFAR10('./CIFAR10_data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test = CIFAR10('./CIFAR10_data', train=False, download=True, transform=transforms.ToTensor())\n",
    "train_CIFAR10_data = train.data; train_CIFAR10_labels = train.targets\n",
    "test_CIFAR10_data = test.data; test_CIFAR10_labels = test.targets\n",
    "\n",
    "print()\n",
    "print(\"MNIST is already an array\")\n",
    "print(train_MNIST_data.shape, train_MNIST_labels.shape, test_MNIST_data.shape, test_MNIST_labels.shape)\n",
    "print()\n",
    "print(\"CIFAR10 is a list of arrays\")\n",
    "print(len(train_CIFAR10_data), len(train_CIFAR10_labels), len(test_CIFAR10_data), len(test_CIFAR10_labels))\n",
    "print(train_CIFAR10_data[0].shape, test_CIFAR10_data[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3JjIF9UdNyW"
   },
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "executionInfo": {
     "elapsed": 2730,
     "status": "ok",
     "timestamp": 1611975285197,
     "user": {
      "displayName": "Prasad Narahari Raghavendra",
      "photoUrl": "",
      "userId": "13004936298388256166"
     },
     "user_tz": 300
    },
    "id": "2P-GNkhPdNyX"
   },
   "outputs": [],
   "source": [
    "class CIFAR10Dataset(data.Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        X = np.transpose(self.X[index], (2, 0, 1)) / 255\n",
    "        X = X.astype(float)\n",
    "        Y = self.Y[index]\n",
    "        return X,Y\n",
    "\n",
    "class MNIST_Dataset(data.Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        X = np.pad(self.X[index], 2) / 255\n",
    "        X = np.repeat(X[:, :, np.newaxis], 3, axis = 2)\n",
    "        X = np.transpose(X, (2, 0, 1))\n",
    "        X = X.astype(float)\n",
    "        Y = self.Y[index]\n",
    "        return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiFk7X_edNyZ"
   },
   "source": [
    "Using the torch.utils.data DataLoader, we shuffle the data and set the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "executionInfo": {
     "elapsed": 2727,
     "status": "ok",
     "timestamp": 1611975285199,
     "user": {
      "displayName": "Prasad Narahari Raghavendra",
      "photoUrl": "",
      "userId": "13004936298388256166"
     },
     "user_tz": 300
    },
    "id": "R-RXL-JJdNyZ"
   },
   "outputs": [],
   "source": [
    "num_workers = 8 if cuda else 0 \n",
    "batch_size = 256\n",
    "    \n",
    "# MNIST Training\n",
    "train_dataset = MNIST_Dataset(train_MNIST_data, train_MNIST_labels)\n",
    "\n",
    "train_loader_args = dict(shuffle=True, batch_size=batch_size, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=batch_size)\n",
    "train_MNIST_loader = data.DataLoader(train_dataset, **train_loader_args)\n",
    "\n",
    "# MNIST Testing\n",
    "test_dataset = MNIST_Dataset(test_MNIST_data, test_MNIST_labels)\n",
    "\n",
    "test_loader_args = dict(shuffle=False, batch_size=batch_size, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=False, batch_size=1)\n",
    "test_MNIST_loader = data.DataLoader(test_dataset, **test_loader_args)\n",
    "\n",
    "# CIFAR10 Training\n",
    "train_dataset = CIFAR10Dataset(train_CIFAR10_data, train_CIFAR10_labels)\n",
    "\n",
    "train_loader_args = dict(shuffle=True, batch_size=batch_size, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=batch_size)\n",
    "train_CIFAR10_loader = data.DataLoader(train_dataset, **train_loader_args)\n",
    "\n",
    "# CIFAR10 Testing\n",
    "test_dataset = CIFAR10Dataset(test_CIFAR10_data, test_CIFAR10_labels)\n",
    "\n",
    "test_loader_args = dict(shuffle=False, batch_size=batch_size, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=False, batch_size=1)\n",
    "test_CIFAR10_loader = data.DataLoader(test_dataset, **test_loader_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tlhg9dfadNyZ"
   },
   "source": [
    "## Define our Neural Network Model \n",
    "We define our model using the torch.nn.Module class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "executionInfo": {
     "elapsed": 2725,
     "status": "ok",
     "timestamp": 1611975285201,
     "user": {
      "displayName": "Prasad Narahari Raghavendra",
      "photoUrl": "",
      "userId": "13004936298388256166"
     },
     "user_tz": 300
    },
    "id": "1rWxDIX7dNyZ"
   },
   "outputs": [],
   "source": [
    "class MyCNN_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCNN_Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size = 5)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size = 2)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size = 5)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = 2)\n",
    "        self.conv3 = nn.Conv2d(32, 10, kernel_size = 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(-1, 10)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVzzuB_IdNya"
   },
   "source": [
    "## Create the model and define the Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2723,
     "status": "ok",
     "timestamp": 1611975285203,
     "user": {
      "displayName": "Prasad Narahari Raghavendra",
      "photoUrl": "",
      "userId": "13004936298388256166"
     },
     "user_tz": 300
    },
    "id": "SgGr79Z6dNyc",
    "outputId": "941f3979-56fc-4fe3-a915-7a7348500c5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCNN_Model(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(32, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model = MyCNN_Model()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This train_epoch is the most important function. The idea is to try not to lose momentum just because of some abrupt local minimum (when the loss surface is not smooth). The idea is to make sure that we check two things:\n",
    "### 1. The normal loss optimisation\n",
    "### 2. Uniform noise added to gradients\n",
    "### For each batch (in any given epoch), we pick the one of the two that produces lower loss values.\n",
    "### The momentum of the optimiser though is maintained as per the gradients picked: Either the raw gradients or the gradients added with noise (as we do not want to lose momentum just because of abrupt local minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "executionInfo": {
     "elapsed": 2881,
     "status": "ok",
     "timestamp": 1611975285367,
     "user": {
      "displayName": "Prasad Narahari Raghavendra",
      "photoUrl": "",
      "userId": "13004936298388256166"
     },
     "user_tz": 300
    },
    "id": "F5gYYVQidNyc"
   },
   "outputs": [],
   "source": [
    "def train_epoch(eta, model, train_loader, criterion):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    loss_den = 1\n",
    "    \n",
    "    start_time = time.time()\n",
    "    optimiser = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "    \n",
    "        #previous model\n",
    "        outputs = model(data.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_predictions = target.size(0)\n",
    "        correct_predictions = (predicted == target).sum().item()\n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "        \n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        #convGrad is the set of old gradients\n",
    "        conv1grad = model.conv1.weight.grad\n",
    "        conv2grad = model.conv2.weight.grad\n",
    "        conv3grad = model.conv3.weight.grad\n",
    "                \n",
    "        noisyGrad1 = eta * np.abs(conv1grad.detach().cpu().numpy())\n",
    "        noisyGrad2 = eta * np.abs(conv2grad.detach().cpu().numpy())\n",
    "        noisyGrad3 = eta * np.abs(conv3grad.detach().cpu().numpy())\n",
    "        \n",
    "        newGrad1 = conv1grad + torch.from_numpy(np.random.uniform(-noisyGrad1, noisyGrad1))\n",
    "        newGrad2 = conv2grad + torch.from_numpy(np.random.uniform(-noisyGrad2, noisyGrad2))\n",
    "        newGrad3 = conv3grad + torch.from_numpy(np.random.uniform(-noisyGrad3, noisyGrad3))\n",
    "        \n",
    "        model.conv1.weight.grad = nn.Parameter(torch.from_numpy(newGrad1.detach().numpy()).float())\n",
    "        model.conv2.weight.grad = nn.Parameter(torch.from_numpy(newGrad2.detach().numpy()).float())\n",
    "        model.conv3.weight.grad = nn.Parameter(torch.from_numpy(newGrad3.detach().numpy()).float())\n",
    "        \n",
    "        #The new loss value for the new gradients is computed\n",
    "        outputs = model(data.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_predictions = target.size(0)\n",
    "        correct_predictions = (predicted == target).sum().item()\n",
    "        acc_new = (correct_predictions/total_predictions)*100.0\n",
    "        \n",
    "        loss_new = criterion(outputs, target)\n",
    "        loss_den += 1\n",
    "\n",
    "        #calculuating confusion matrix\n",
    "        predictions += list(predicted.detach().cpu().numpy())\n",
    "        ground_truth += list(target.detach().cpu().numpy())\n",
    "\n",
    "        if loss_new.item() > loss.item():\n",
    "            model.conv1.weight.grad = conv1grad\n",
    "            model.conv2.weight.grad = conv2grad\n",
    "            model.conv3.weight.grad = conv3grad\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        else:\n",
    "            running_loss += loss_new.item()\n",
    "        \n",
    "    end_time = time.time()\n",
    "\n",
    "    running_loss /= loss_den\n",
    "    \n",
    "    print('Training Loss: ', running_loss, 'Time: ',end_time - start_time, 's')\n",
    "    \n",
    "    return running_loss, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5bSz-zcdNyd"
   },
   "source": [
    "## Create a function that will evaluate our network's performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "executionInfo": {
     "elapsed": 2879,
     "status": "ok",
     "timestamp": 1611975285370,
     "user": {
      "displayName": "Prasad Narahari Raghavendra",
      "photoUrl": "",
      "userId": "13004936298388256166"
     },
     "user_tz": 300
    },
    "id": "rAhuZ7uMdNyd"
   },
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0\n",
    "        \n",
    "        predictions = []\n",
    "        ground_truth = []\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):   \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            outputs = model(data.float())\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_predictions += target.size(0)\n",
    "            correct_predictions += (predicted == target).sum().item()\n",
    "\n",
    "            loss = criterion(outputs, target).detach()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            #calculuating confusion matrix\n",
    "            predictions += list(predicted.detach().cpu().numpy())\n",
    "            ground_truth += list(target.detach().cpu().numpy())\n",
    "        \n",
    "        #write_confusion_matrix('Testing', ground_truth, predictions)\n",
    "        running_loss /= len(test_loader)\n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "        print('Testing Loss: ', running_loss)\n",
    "        print('Testing Accuracy: ', acc, '%')\n",
    "        return running_loss, acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sF9evUwmdNye"
   },
   "source": [
    "## Train the model for N epochs\n",
    "We call our training and testing functions in a loop, while keeping track of the losses and accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cjbgMQIFdNye",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  2.982338696465654 Time:  58.64115643501282 s\n",
      "Testing Loss:  2.4523444892406463\n",
      "Testing Accuracy:  10.32 %\n",
      "====================\n",
      "Training Loss:  2.495215079541934 Time:  57.059688091278076 s\n",
      "Testing Loss:  2.6800487677812574\n",
      "Testing Accuracy:  10.32 %\n",
      "====================\n",
      "Training Loss:  3.1331407286353032 Time:  57.16563701629639 s\n",
      "Testing Loss:  3.6217195371329782\n",
      "Testing Accuracy:  10.09 %\n",
      "====================\n",
      "Training Loss:  5.119678274049598 Time:  57.953062534332275 s\n",
      "Testing Loss:  8.410976375210286\n",
      "Testing Accuracy:  9.8 %\n",
      "====================\n",
      "Training Loss:  2.3521221628043856 Time:  46.376412868499756 s\n",
      "Testing Loss:  2.3259525919675825\n",
      "Testing Accuracy:  10.01 %\n",
      "--------------------\n",
      "Training Loss:  2.330018611123719 Time:  47.75367259979248 s\n",
      "Testing Loss:  2.3980944871902468\n",
      "Testing Accuracy:  10.0 %\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 4\n",
    "eta = 0.01\n",
    "\n",
    "model = MyCNN_Model(); model.to(device)\n",
    "for i in range(n_epochs):\n",
    "    train_loss, model = train_epoch(eta, model, train_MNIST_loader, criterion)\n",
    "    test_loss, MNIST_test_acc = test_model(model, test_MNIST_loader, criterion)\n",
    "    print('='*20)\n",
    "\n",
    "model = MyCNN_Model(); model.to(device)\n",
    "for i in range(n_epochs):\n",
    "    train_loss, model = train_epoch(eta, model, train_CIFAR10_loader, criterion)\n",
    "    test_loss, CIFAR10_test_acc = test_model(model, test_CIFAR10_loader, criterion)\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CNN_Gradient_Check.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
